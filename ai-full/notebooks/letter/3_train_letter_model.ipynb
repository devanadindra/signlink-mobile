{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c6a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total samples (termasuk Augmentasi): 22368\n",
      "📂 Total classes: 26\n",
      "✅ X shape after reshape for CNN: (22368, 225, 1)\n",
      "✅ y shape: (22368, 26)\n",
      "Train size: 17894, Test size: 4474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 223, 128)          512       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 223, 128)          512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 111, 128)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 109, 256)          98560     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 109, 256)          1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 54, 256)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 52, 512)           393728    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 512)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                6682      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 632346 (2.41 MB)\n",
      "Trainable params: 631578 (2.41 MB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 01:32:43.560867: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 7s 24ms/step - loss: 4.3948 - accuracy: 0.0464 - val_loss: 4.0369 - val_accuracy: 0.0566 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 4.2678 - accuracy: 0.0510 - val_loss: 3.6463 - val_accuracy: 0.0596 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.9677 - accuracy: 0.0522 - val_loss: 3.3760 - val_accuracy: 0.0592 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.7531 - accuracy: 0.0538 - val_loss: 3.3773 - val_accuracy: 0.0577 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.6081 - accuracy: 0.0571 - val_loss: 3.4118 - val_accuracy: 0.0641 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.5135 - accuracy: 0.0614 - val_loss: 3.5276 - val_accuracy: 0.0544 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.4472 - accuracy: 0.0606 - val_loss: 3.3380 - val_accuracy: 0.0696 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.4127 - accuracy: 0.0664 - val_loss: 3.3503 - val_accuracy: 0.0723 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.4112 - accuracy: 0.0636 - val_loss: 3.2938 - val_accuracy: 0.0715 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.3619 - accuracy: 0.0688 - val_loss: 3.2147 - val_accuracy: 0.0771 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.3331 - accuracy: 0.0679 - val_loss: 3.2047 - val_accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.3286 - accuracy: 0.0678 - val_loss: 3.1346 - val_accuracy: 0.0823 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.2771 - accuracy: 0.0748 - val_loss: 3.1616 - val_accuracy: 0.0823 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.2482 - accuracy: 0.0801 - val_loss: 3.1088 - val_accuracy: 0.0920 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.2589 - accuracy: 0.0781 - val_loss: 3.2006 - val_accuracy: 0.0804 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.2409 - accuracy: 0.0827 - val_loss: 3.1584 - val_accuracy: 0.0924 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.2464 - accuracy: 0.0884 - val_loss: 3.1851 - val_accuracy: 0.0901 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.2660 - accuracy: 0.0859 - val_loss: 3.1515 - val_accuracy: 0.0819 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.2799 - accuracy: 0.0876 - val_loss: 3.2934 - val_accuracy: 0.0872 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.2678 - accuracy: 0.0958 - val_loss: 3.2432 - val_accuracy: 0.0793 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.3121 - accuracy: 0.0892 - val_loss: 3.2207 - val_accuracy: 0.1043 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "237/238 [============================>.] - ETA: 0s - loss: 3.3422 - accuracy: 0.0920\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.3423 - accuracy: 0.0921 - val_loss: 3.2280 - val_accuracy: 0.1032 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1826 - accuracy: 0.1043 - val_loss: 3.0992 - val_accuracy: 0.1035 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1649 - accuracy: 0.1051 - val_loss: 3.0752 - val_accuracy: 0.1061 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1583 - accuracy: 0.1121 - val_loss: 3.1163 - val_accuracy: 0.1102 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1987 - accuracy: 0.1028 - val_loss: 3.1052 - val_accuracy: 0.1110 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1798 - accuracy: 0.1082 - val_loss: 3.1515 - val_accuracy: 0.1058 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.1773 - accuracy: 0.1070 - val_loss: 3.1410 - val_accuracy: 0.0994 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1839 - accuracy: 0.1099 - val_loss: 3.1873 - val_accuracy: 0.1073 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 3.2018 - accuracy: 0.1095 - val_loss: 3.1432 - val_accuracy: 0.1091 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.2255 - accuracy: 0.1121 - val_loss: 3.1450 - val_accuracy: 0.1147 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "237/238 [============================>.] - ETA: 0s - loss: 3.2305 - accuracy: 0.1107\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.2300 - accuracy: 0.1108 - val_loss: 3.1467 - val_accuracy: 0.1061 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1328 - accuracy: 0.1184 - val_loss: 3.0271 - val_accuracy: 0.1210 - lr: 2.5000e-04\n",
      "Epoch 34/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1087 - accuracy: 0.1192 - val_loss: 3.0754 - val_accuracy: 0.1132 - lr: 2.5000e-04\n",
      "Epoch 35/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.0898 - accuracy: 0.1274 - val_loss: 3.0145 - val_accuracy: 0.1300 - lr: 2.5000e-04\n",
      "Epoch 36/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.1160 - accuracy: 0.1218 - val_loss: 3.0495 - val_accuracy: 0.1270 - lr: 2.5000e-04\n",
      "Epoch 37/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1120 - accuracy: 0.1222 - val_loss: 3.0578 - val_accuracy: 0.1181 - lr: 2.5000e-04\n",
      "Epoch 38/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.1100 - accuracy: 0.1253 - val_loss: 3.1229 - val_accuracy: 0.1203 - lr: 2.5000e-04\n",
      "Epoch 39/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.1229 - accuracy: 0.1188 - val_loss: 3.0931 - val_accuracy: 0.1095 - lr: 2.5000e-04\n",
      "Epoch 40/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.1207 - accuracy: 0.1223 - val_loss: 3.0678 - val_accuracy: 0.1181 - lr: 2.5000e-04\n",
      "Epoch 41/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.1249 - accuracy: 0.1266 - val_loss: 3.1492 - val_accuracy: 0.1181 - lr: 2.5000e-04\n",
      "Epoch 42/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.1638 - accuracy: 0.1226 - val_loss: 3.0193 - val_accuracy: 0.1236 - lr: 2.5000e-04\n",
      "Epoch 43/200\n",
      "237/238 [============================>.] - ETA: 0s - loss: 3.1324 - accuracy: 0.1226\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.1337 - accuracy: 0.1224 - val_loss: 3.2192 - val_accuracy: 0.1099 - lr: 2.5000e-04\n",
      "Epoch 44/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.0756 - accuracy: 0.1295 - val_loss: 3.0477 - val_accuracy: 0.1192 - lr: 1.2500e-04\n",
      "Epoch 45/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.0706 - accuracy: 0.1290 - val_loss: 3.0129 - val_accuracy: 0.1300 - lr: 1.2500e-04\n",
      "Epoch 46/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.0726 - accuracy: 0.1274 - val_loss: 3.0498 - val_accuracy: 0.1218 - lr: 1.2500e-04\n",
      "Epoch 47/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.0620 - accuracy: 0.1293 - val_loss: 3.1520 - val_accuracy: 0.1102 - lr: 1.2500e-04\n",
      "Epoch 48/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.0808 - accuracy: 0.1307 - val_loss: 3.0750 - val_accuracy: 0.1210 - lr: 1.2500e-04\n",
      "Epoch 49/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.0815 - accuracy: 0.1294 - val_loss: 3.0701 - val_accuracy: 0.1173 - lr: 1.2500e-04\n",
      "Epoch 50/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.0847 - accuracy: 0.1310 - val_loss: 3.1203 - val_accuracy: 0.1203 - lr: 1.2500e-04\n",
      "Epoch 51/200\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 3.0752 - accuracy: 0.1295 - val_loss: 3.0669 - val_accuracy: 0.1214 - lr: 1.2500e-04\n",
      "Epoch 52/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.0913 - accuracy: 0.1312 - val_loss: 3.1110 - val_accuracy: 0.1214 - lr: 1.2500e-04\n",
      "Epoch 53/200\n",
      "237/238 [============================>.] - ETA: 0s - loss: 3.0856 - accuracy: 0.1289\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.0859 - accuracy: 0.1287 - val_loss: 3.1538 - val_accuracy: 0.1196 - lr: 1.2500e-04\n",
      "Epoch 54/200\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.0626 - accuracy: 0.1315 - val_loss: 3.0840 - val_accuracy: 0.1236 - lr: 6.2500e-05\n",
      "Epoch 55/200\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 3.0430 - accuracy: 0.1358 - val_loss: 3.1059 - val_accuracy: 0.1192 - lr: 6.2500e-05\n",
      "Epoch 56/200\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 3.0493 - accuracy: 0.1376 - val_loss: 3.1124 - val_accuracy: 0.1181 - lr: 6.2500e-05\n",
      "Epoch 57/200\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 3.0414 - accuracy: 0.1314 - val_loss: 3.0907 - val_accuracy: 0.1207 - lr: 6.2500e-05\n",
      "Epoch 58/200\n",
      "238/238 [==============================] - 6s 23ms/step - loss: 3.0411 - accuracy: 0.1366 - val_loss: 3.1204 - val_accuracy: 0.1143 - lr: 6.2500e-05\n",
      "Epoch 59/200\n",
      "238/238 [==============================] - 6s 23ms/step - loss: 3.0427 - accuracy: 0.1349 - val_loss: 3.1048 - val_accuracy: 0.1188 - lr: 6.2500e-05\n",
      "Epoch 60/200\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 3.0549 - accuracy: 0.1301 - val_loss: 3.0949 - val_accuracy: 0.1225 - lr: 6.2500e-05\n",
      "Epoch 61/200\n",
      "238/238 [==============================] - ETA: 0s - loss: 3.0528 - accuracy: 0.1357\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 3.0528 - accuracy: 0.1357 - val_loss: 3.1144 - val_accuracy: 0.1173 - lr: 6.2500e-05\n",
      "Epoch 62/200\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 3.0304 - accuracy: 0.1400 - val_loss: 3.1070 - val_accuracy: 0.1177 - lr: 3.1250e-05\n",
      "Epoch 63/200\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 3.0297 - accuracy: 0.1330 - val_loss: 3.1128 - val_accuracy: 0.1177 - lr: 3.1250e-05\n",
      "Epoch 64/200\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 3.0207 - accuracy: 0.1399 - val_loss: 3.1082 - val_accuracy: 0.1207 - lr: 3.1250e-05\n",
      "Epoch 65/200\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 3.0095 - accuracy: 0.1375 - val_loss: 3.0871 - val_accuracy: 0.1210 - lr: 3.1250e-05\n",
      "✅ Model 1D-CNN huruf berhasil disimpan di folder '../../models/'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3_train_letter_model.ipynb\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# =========================================================================\n",
    "# 1️⃣ Load dataset processed + Augmentasi (Menggunakan Augmentasi Lebih Baik)\n",
    "# =========================================================================\n",
    "X, y = [], []\n",
    "data_dir = \"../../dataset/processed_letters\"\n",
    "# Definisikan parameter augmentasi\n",
    "NOISE_RANGE = 0.03 # Sedikit ditingkatkan\n",
    "AUG_FACTOR = 3     # Melipatgandakan data 3 kali\n",
    "for label in os.listdir(data_dir):\n",
    "    folder = os.path.join(data_dir, label)\n",
    "    if not os.path.isdir(folder):\n",
    "        continue\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".npy\"):\n",
    "            data = np.load(os.path.join(folder, file), allow_pickle=True)\n",
    "            if len(data) == 0: continue\n",
    "\n",
    "            # 1. Sampel asli\n",
    "            X.append(data)\n",
    "            y.append(label)\n",
    "\n",
    "            # 2. Augmentasi N kali\n",
    "            for _ in range(AUG_FACTOR):\n",
    "                # Augmentasi: noise Gaussian untuk distribusi yang lebih alami\n",
    "                noise = np.random.normal(0, NOISE_RANGE, data.shape) \n",
    "                data_aug = data + noise\n",
    "                X.append(data_aug)\n",
    "                y.append(label)\n",
    "X = np.array(X, dtype='float32')\n",
    "y = np.array(y)\n",
    "print(f\"✅ Total samples (termasuk Augmentasi): {len(X)}\")\n",
    "print(f\"📂 Total classes: {len(set(y))}\")\n",
    "# ==========================================\n",
    "# 2️⃣ Encode label & 3️⃣ Normalisasi + Reshape\n",
    "# ==========================================\n",
    "le = LabelEncoder()\n",
    "y_encoded = to_categorical(le.fit_transform(y))\n",
    "num_classes = y_encoded.shape[1]\n",
    "# Gunakan StandardScaler untuk normalisasi\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Reshape untuk 1D-CNN: (samples, features, 1)\n",
    "X_cnn = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1) \n",
    "print(f\"✅ X shape after reshape for CNN: {X_cnn.shape}\")\n",
    "print(f\"✅ y shape: {y_encoded.shape}\")\n",
    "# ==========================================\n",
    "# 4️⃣ Split dataset\n",
    "# ==========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cnn, y_encoded, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "# ==========================================\n",
    "# 5️⃣ Buat Model 1D-CNN (Arsitektur Diperkaya)\n",
    "# ==========================================\n",
    "input_shape = (X_cnn.shape[1], 1)\n",
    "model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=512, kernel_size=3, activation='relu'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# ==========================================\n",
    "# 6️⃣ Training dengan Advanced Callbacks\n",
    "# ==========================================\n",
    "es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=0.00001, verbose=1)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[es, reduce_lr],\n",
    "    shuffle=True\n",
    ")\n",
    "# ==========================================\n",
    "# 7️⃣ Simpan model, label encoder, scaler\n",
    "# ==========================================\n",
    "os.makedirs(\"../../models\", exist_ok=True)\n",
    "model.save(\"../../models/sign_letter_model.keras\")\n",
    "with open(\"../../models/label_letter_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "with open(\"../../models/scaler_letter.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"✅ Model 1D-CNN huruf berhasil disimpan di folder '../../models/'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
