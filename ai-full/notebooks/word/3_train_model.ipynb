{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total samples: 10\n",
      "ðŸ“‚ Total classes: 2\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 30, 64)            74240     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               98816     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173314 (677.01 KB)\n",
      "Trainable params: 173314 (677.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6403 - accuracy: 0.7500 - val_loss: 1.0826 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 0.5456 - accuracy: 0.7500 - val_loss: 1.6585 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 1s 805ms/step - loss: 0.5049 - accuracy: 0.7500 - val_loss: 1.7095 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 0.4796 - accuracy: 0.7500 - val_loss: 1.3522 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 1s 758ms/step - loss: 0.4292 - accuracy: 0.7500 - val_loss: 1.1237 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 1s 763ms/step - loss: 0.3894 - accuracy: 0.7500 - val_loss: 1.0686 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 1s 766ms/step - loss: 0.3187 - accuracy: 0.7500 - val_loss: 1.1264 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 0.2823 - accuracy: 0.7500 - val_loss: 0.8084 - val_accuracy: 0.5000\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 1s 780ms/step - loss: 0.2026 - accuracy: 0.8750 - val_loss: 0.5116 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 1s 751ms/step - loss: 0.1533 - accuracy: 1.0000 - val_loss: 0.3442 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 1s 815ms/step - loss: 0.0917 - accuracy: 1.0000 - val_loss: 0.1837 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 1s 754ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 1s 761ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 1s 758ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 6.4969e-06 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 1.9818e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 1s 753ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 1s 773ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.5838 - val_accuracy: 0.5000\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 1s 775ms/step - loss: 2.8035 - accuracy: 0.8750 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 1s 829ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 379.2558 - accuracy: 0.2500 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 1s 804ms/step - loss: 151.8316 - accuracy: 0.2500 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 45.8200 - accuracy: 0.3750 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 1s 750ms/step - loss: 23.1663 - accuracy: 0.6250 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 1s 763ms/step - loss: 14.7033 - accuracy: 0.6250 - val_loss: 10.1008 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 2.7550 - accuracy: 0.5000 - val_loss: 14.5842 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 1s 822ms/step - loss: 3.8522 - accuracy: 0.7500 - val_loss: 22.6145 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 1s 779ms/step - loss: 6.0448 - accuracy: 0.7500 - val_loss: 94.4905 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 24.9175 - accuracy: 0.7500 - val_loss: 49.3506 - val_accuracy: 0.5000\n",
      "âœ… Model dan label encoder berhasil disimpan di folder '../models/'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3_train_model.ipynb\n",
    "# Training model pengenalan bahasa isyarat\n",
    "# ==========================================\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset dari folder processed\n",
    "X, y = [], []\n",
    "data_dir = \"../../dataset/processed_words\"\n",
    "\n",
    "for label in os.listdir(data_dir):\n",
    "    folder = os.path.join(data_dir, label)\n",
    "    if not os.path.isdir(folder):\n",
    "        continue\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".npy\"):\n",
    "            data = np.load(os.path.join(folder, file), allow_pickle=True)\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            X.append(data)\n",
    "            y.append(label)\n",
    "\n",
    "print(f\"âœ… Total samples: {len(X)}\")\n",
    "print(f\"ðŸ“‚ Total classes: {len(set(y))}\")\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "y_encoded = to_categorical(le.fit_transform(y))\n",
    "\n",
    "# Padding biar semua sequence punya panjang sama\n",
    "X_padded = pad_sequences(X, maxlen=30, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Buat model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, activation='relu', input_shape=(30, X_padded.shape[2])),\n",
    "    LSTM(128, activation='relu'),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training model\n",
    "model.fit(X_padded, y_encoded, epochs=30, batch_size=8, validation_split=0.2)\n",
    "\n",
    "# Simpan model ke format .keras (modern)\n",
    "os.makedirs(\"../../models\", exist_ok=True)\n",
    "model.save(\"../../models/sign_word_model.keras\")\n",
    "\n",
    "# Simpan label encoder untuk inference nanti\n",
    "import pickle\n",
    "with open(\"../../models/label_word_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"âœ… Model dan label encoder berhasil disimpan di folder '../models/'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
