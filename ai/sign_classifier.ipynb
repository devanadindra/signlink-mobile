{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397381d5",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efbb9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 479.6 MB 73 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting tf2onnx\n",
      "  Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
      "\u001b[K     |████████████████████████████████| 455 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 67.0 MB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 229 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.70.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: setuptools in /mnt/f/SignLink/signlink-mobile/aienv/lib/python3.8/site-packages (from tensorflow) (44.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /mnt/f/SignLink/signlink-mobile/aienv/lib/python3.8/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[K     |████████████████████████████████| 440 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.3-cp38-cp38-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[K     |████████████████████████████████| 135 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /mnt/f/SignLink/signlink-mobile/aienv/lib/python3.8/site-packages (from tensorflow) (25.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnx>=1.4.1\n",
      "  Downloading onnx-1.17.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.0 MB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0; python_version < \"3.10\"\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/f/SignLink/signlink-mobile/aienv/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.57.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.41.1-py2.py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /mnt/f/SignLink/signlink-mobile/aienv/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.20.2)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /mnt/f/SignLink/signlink-mobile/aienv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (8.5.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25h\u001b[31mERROR: tensorflow 2.13.1 has requirement numpy<=1.24.3,>=1.22, but you'll have numpy 1.24.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tf2onnx 1.16.1 has requirement protobuf~=3.20, but you'll have protobuf 4.25.8 which is incompatible.\u001b[0m\n",
      "Installing collected packages: flatbuffers, google-pasta, protobuf, wheel, astunparse, numpy, h5py, urllib3, charset-normalizer, certifi, idna, requests, absl-py, MarkupSafe, werkzeug, markdown, tensorboard-data-server, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, tensorboard, opt-einsum, termcolor, gast, tensorflow-estimator, wrapt, keras, tensorflow-io-gcs-filesystem, typing-extensions, libclang, tensorflow, onnx, tf2onnx, importlib-resources, cycler, fonttools, pillow, contourpy, pyparsing, kiwisolver, matplotlib, opencv-python, scipy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.13.2\n",
      "    Uninstalling typing-extensions-4.13.2:\n",
      "      Successfully uninstalled typing-extensions-4.13.2\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.3.1 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.10.5 charset-normalizer-3.4.4 contourpy-1.1.1 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.57.0 gast-0.4.0 google-auth-2.41.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.11.0 idna-3.11 importlib-resources-6.4.5 keras-2.13.1 kiwisolver-1.4.7 libclang-18.1.1 markdown-3.7 matplotlib-3.7.5 numpy-1.24.4 oauthlib-3.3.1 onnx-1.17.0 opencv-python-4.12.0.88 opt-einsum-3.4.0 pillow-10.4.0 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyparsing-3.1.4 requests-2.32.4 requests-oauthlib-2.0.0 rsa-4.9.1 scipy-1.10.1 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 tf2onnx-1.16.1 typing-extensions-4.5.0 urllib3-2.2.3 werkzeug-3.0.6 wheel-0.45.1 wrapt-1.17.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow tf2onnx matplotlib numpy opencv-python scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed27fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall tensorflow tf2onnx matplotlib numpy opencv-python scipy mediapipe pandas tqdm scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d18590",
   "metadata": {},
   "source": [
    "# import and constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from tensorflow.keras.applications import MobileNetV2 # type: ignore\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow.keras.preprocessing import image # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import cv2 # type: ignore\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import cv2 # type: ignore\n",
    "import mediapipe as mp # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.preprocessing import LabelEncoder # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "import joblib # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be719935",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './dataset'\n",
    "model_name = 'sign_classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13103e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766be0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./alfabet-bisindo.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/achmadnoer/alfabet-bisindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66037a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./alfabet-bisindo.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\"❌ File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\"📦 Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"✅ Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️ OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21028c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_dir = \"Citra BISINDO\"\n",
    "dst_dir = \"./dataset\"\n",
    "\n",
    "if not os.path.exists(src_dir):\n",
    "    raise FileNotFoundError(f\"❌ Folder sumber tidak ditemukan: {src_dir}\")\n",
    "\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Memindahkan '{src_dir}' ke '{dst_dir}' ...\")\n",
    "shutil.move(src_dir, dst_dir)\n",
    "print(f\"✅ Berhasil dipindahkan ke {dst_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./indonesian-sign-language-bisindo.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/agungmrf/indonesian-sign-language-bisindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure.\n",
    "    Example: src_folder/cat -> dataset/cat\n",
    "             src_folder/dog -> dataset/dog\n",
    "    \"\"\"\n",
    "    # Ensure destination exists\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over all label folders in the source\n",
    "    for label in os.listdir(src_folder):\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Skip if not a folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Create label folder in destination if needed\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Move all files from src → dest\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Avoid overwriting files with same name\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_2{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\"✅ Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./indonesian-sign-language-bisindo.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\"❌ File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\"📦 Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"✅ Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️ OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset('./bisindo/images/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset('./bisindo/images/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1353535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./bisindo\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"🗑️ Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\"✅ Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\"⚠️ Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeabb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./bisindo-dataset.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/yunitayupratiwi/bisindo-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./bisindo-dataset.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\"❌ File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\"📦 Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"✅ Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️ OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, based on the first letter of the filename.\n",
    "    Example: src_folder/A.66ae97e2-c1e4-11eb-83d3-0008ca6b6d30.jpg -> dataset/A\n",
    "             src_folder/B.002d8fdf-c1e3-11eb-952a-0008ca6b6d30.jpg -> dataset/B\n",
    "    \"\"\"\n",
    "    # Ensure destination exists\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over all file folders in the source\n",
    "    for filename in os.listdir(src_folder):\n",
    "        src_file = os.path.join(src_folder, filename)\n",
    "\n",
    "        # Skip if a folder\n",
    "        if os.path.isdir(src_file):\n",
    "            continue\n",
    "        \n",
    "        # Skip if not a jpg\n",
    "        if not src_file.lower().endswith('.jpg'):\n",
    "            continue\n",
    "        \n",
    "        label = filename[0].upper()  # First character as label\n",
    "        dest = os.path.join(dataset_dir, label)\n",
    "        # Create label folder in destination if needed\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "        dst_file = os.path.join(dest, filename)\n",
    "\n",
    "        # Avoid overwriting files with same name\n",
    "        if os.path.exists(dst_file):\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            dst_file = os.path.join(dest, f\"{base}_3{ext}\")\n",
    "\n",
    "        shutil.move(src_file, dst_file)\n",
    "\n",
    "    print(f\"successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b15a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_dataset(\"./BISINDO - Dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf12b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_dataset(\"./BISINDO - Dataset/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./BISINDO - Dataset\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"🗑️ Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\"✅ Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\"⚠️ Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./sign-language-bisindo.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/bonarsitorus/sign-language-bisindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e87a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./sign-language-bisindo.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\"❌ File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\"📦 Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"✅ Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️ OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f178eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset_4(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure,\n",
    "    except folders with '_npy' in their name.\n",
    "    \"\"\"\n",
    "    # Pastikan folder tujuan ada\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterasi semua folder di src_folder\n",
    "    for label in os.listdir(src_folder):\n",
    "        # Lewati folder yang mengandung '_npy'\n",
    "        if \"_npy\" in label:\n",
    "            continue\n",
    "\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Pastikan ini folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Buat folder di tujuan jika belum ada\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Pindahkan semua file\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Hindari overwrite\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_4{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\"✅ Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_4('./data_tambahan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3406b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./data_tambahan\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"🗑️ Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\"✅ Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\"⚠️ Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./indonesian-hand-sign-language-bisindo-dataset.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/kelsha/indonesian-hand-sign-language-bisindo-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./indonesian-hand-sign-language-bisindo-dataset.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\"❌ File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\"📦 Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"✅ Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️ OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset_5(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure.\n",
    "    \"\"\"\n",
    "    # Pastikan folder tujuan ada\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterasi semua folder di src_folder\n",
    "    for label in os.listdir(src_folder):\n",
    "\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Pastikan ini folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Buat folder di tujuan jika belum ada\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Pindahkan semua file\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Hindari overwrite\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_5{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\"✅ Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_5('./dataset_bisindo/train/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d39e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_5('./dataset_bisindo/val/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1beb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./dataset_bisindo\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"🗑️ Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\"✅ Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\"⚠️ Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./bisindo-final.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/skripsiairlangga/bisindo-final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ca9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./bisindo-final.zip\"\n",
    "extract_dir = \"./bisindo_final\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\"❌ File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "# Membuat folder extract_dir jika belum ada\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📦 Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"✅ Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️ OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57255468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset_6(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure.\n",
    "    \"\"\"\n",
    "    # Pastikan folder tujuan ada\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterasi semua folder di src_folder\n",
    "    for label in os.listdir(src_folder):\n",
    "\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Pastikan ini folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Buat folder di tujuan jika belum ada\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Pindahkan semua file\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Hindari overwrite\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_6{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\"✅ Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce715f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_6('./bisindo_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./bisindo_final\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"🗑️ Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\"✅ Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\"⚠️ Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4257553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(image_rgb)\n",
    "    \n",
    "    if not result.multi_hand_landmarks:\n",
    "        return None\n",
    "    \n",
    "    # List semua tangan yang terdeteksi\n",
    "    coords_all = []\n",
    "    for landmarks in result.multi_hand_landmarks:\n",
    "        coords = []\n",
    "        for lm in landmarks.landmark:\n",
    "            coords.extend([lm.x, lm.y, lm.z])\n",
    "        coords_all.append(coords)\n",
    "    \n",
    "    # Jika hanya 1 tangan, tambahkan 0 agar panjang fitur tetap konsisten\n",
    "    if len(coords_all) == 1:\n",
    "        coords_all.append([0.0]*63)  # tangan kosong\n",
    "    \n",
    "    # Flatten dua tangan (kanan + kiri)\n",
    "    features = np.array(coords_all[0] + coords_all[1])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for label in sorted(os.listdir(dataset_dir)):\n",
    "    label_path = os.path.join(dataset_dir, label)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    print(f'Processing label: {label}')\n",
    "    for img_name in tqdm(os.listdir(label_path)):\n",
    "        img_path = os.path.join(label_path, img_name)\n",
    "        landmarks = extract_hand_landmarks(img_path)\n",
    "        if landmarks is not None:\n",
    "            data.append(landmarks)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e16017",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"preprocessing\", exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['label'] = labels\n",
    "\n",
    "csv_path = os.path.join(\"preprocessing\", \"hand_keypoints.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Data dan label berhasil disimpan ke {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeba27f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9411987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 25305\n",
      "          0         1             2         3         4         5         6  \\\n",
      "0  0.589252  0.788577  7.732578e-08  0.540710  0.797574 -0.010122  0.494010   \n",
      "1  0.816977  0.762783 -9.139245e-08  0.801779  0.732191  0.003005  0.774102   \n",
      "2  0.725103  0.491935 -4.427249e-07  0.658864  0.500457 -0.027891  0.584359   \n",
      "3  0.188371  0.656543 -8.581127e-08  0.283549  0.646443 -0.036480  0.366026   \n",
      "4  0.113166  0.822599  5.401650e-07  0.195477  0.813338 -0.077608  0.288585   \n",
      "\n",
      "          7         8         9  ...       117       118       119       120  \\\n",
      "0  0.776065 -0.034902  0.458915  ...  0.259818  0.687959 -0.093514  0.256858   \n",
      "1  0.710470  0.000877  0.747481  ...  0.627653  0.740314 -0.056922  0.625121   \n",
      "2  0.476563 -0.053607  0.521293  ...  0.223792  0.330818 -0.103738  0.219856   \n",
      "3  0.589501 -0.066297  0.421726  ...  0.758943  0.368739 -0.107302  0.773216   \n",
      "4  0.775300 -0.115059  0.381177  ...  0.871711  0.542666 -0.079876  0.875374   \n",
      "\n",
      "        121       122       123       124       125  label  \n",
      "0  0.731087 -0.083111  0.242542  0.740920 -0.070938      A  \n",
      "1  0.737795 -0.057063  0.627400  0.744683 -0.054733      A  \n",
      "2  0.384147 -0.090373  0.220490  0.394726 -0.071387      A  \n",
      "3  0.424807 -0.090161  0.791487  0.425094 -0.069101      A  \n",
      "4  0.603269 -0.072792  0.862440  0.643383 -0.055686      A  \n",
      "\n",
      "[5 rows x 127 columns]\n"
     ]
    }
   ],
   "source": [
    "csv_input_path = os.path.join(\"preprocessing\", \"hand_keypoints.csv\")\n",
    "df = pd.read_csv(csv_input_path)\n",
    "print(f\"Total data: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['label'])\n",
    "X = df.drop('label', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f130e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9753c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e90ded",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db52f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(\"models\", f\"{model_name}.keras\")\n",
    "label_path = os.path.join(\"models\", f\"{model_name}_label.pkl\")\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "with open(label_path, \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(f\"✅ Model disimpan di {model_path}\")\n",
    "print(f\"✅ Label encoder disimpan di {label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf # type: ignore\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = os.path.join(\"models\", f\"{model_name}.tflite\")\n",
    "\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"✅ Model TFLite disimpan di {tflite_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "label_path = os.path.join(\"models\", f\"{model_name}_label.pkl\")\n",
    "with open(label_path, \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "labels = le.classes_\n",
    "\n",
    "txt_path = os.path.join(\"models\", \"labels.txt\")\n",
    "with open(txt_path, \"w\") as f:\n",
    "    for label in labels:\n",
    "        f.write(label + \"\\n\")\n",
    "\n",
    "print(f\"✅ Labels berhasil disimpan di {txt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
