{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397381d5",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efbb9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnxNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (2.2.6)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (1.15.3)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting onnx>=1.4.1 (from tf2onnx)\n",
      "  Using cached onnx-1.19.1-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting requests (from tf2onnx)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: six in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (from tf2onnx) (1.17.0)\n",
      "Collecting flatbuffers>=1.12 (from tf2onnx)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf~=3.20 (from tf2onnx)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl.metadata (698 bytes)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.1-cp310-cp310-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-12.0.0-cp310-cp310-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.1-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.3-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.75.1-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->tf2onnx)\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->tf2onnx)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->tf2onnx)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->tf2onnx)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.17.0-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "INFO: pip is looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting onnx>=1.4.1 (from tf2onnx)\n",
      "  Using cached onnx-1.19.0-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "  Using cached onnx-1.18.0-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "  Using cached onnx-1.17.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\windows 10\\anaconda3\\envs\\aienv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "Using cached matplotlib-3.10.7-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Downloading tensorflow-2.19.1-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "   ---------------------------------------- 0.0/375.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/375.7 MB 11.2 MB/s eta 0:00:34\n",
      "   ---------------------------------------- 2.9/375.7 MB 7.3 MB/s eta 0:00:52\n",
      "   ---------------------------------------- 4.5/375.7 MB 7.4 MB/s eta 0:00:50\n",
      "    --------------------------------------- 7.1/375.7 MB 8.7 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 9.7/375.7 MB 9.4 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 12.3/375.7 MB 9.9 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 14.7/375.7 MB 10.2 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 17.3/375.7 MB 10.4 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 19.9/375.7 MB 10.6 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 22.5/375.7 MB 10.6 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 24.9/375.7 MB 10.7 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 27.5/375.7 MB 10.8 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 29.9/375.7 MB 10.9 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 32.5/375.7 MB 11.0 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 35.1/375.7 MB 11.0 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 37.5/375.7 MB 11.1 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 40.1/375.7 MB 11.1 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 42.7/375.7 MB 11.2 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 45.1/375.7 MB 11.2 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 47.7/375.7 MB 11.3 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 50.1/375.7 MB 11.3 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 52.7/375.7 MB 11.3 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 55.3/375.7 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 57.9/375.7 MB 11.4 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 60.3/375.7 MB 11.4 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 62.9/375.7 MB 11.4 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 65.5/375.7 MB 11.4 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 67.9/375.7 MB 11.4 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 70.5/375.7 MB 11.4 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 73.1/375.7 MB 11.5 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 75.5/375.7 MB 11.5 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 78.1/375.7 MB 11.5 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 80.7/375.7 MB 11.5 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 83.1/375.7 MB 11.5 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 85.5/375.7 MB 11.5 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 88.1/375.7 MB 11.5 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 90.7/375.7 MB 11.5 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 93.3/375.7 MB 11.5 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 95.7/375.7 MB 11.5 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 98.3/375.7 MB 11.6 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 100.9/375.7 MB 11.6 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 103.3/375.7 MB 11.6 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 105.9/375.7 MB 11.6 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 108.5/375.7 MB 11.6 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 110.9/375.7 MB 11.6 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 113.5/375.7 MB 11.6 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 115.9/375.7 MB 11.6 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 118.5/375.7 MB 11.6 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 120.8/375.7 MB 11.6 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 123.5/375.7 MB 11.6 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 126.1/375.7 MB 11.6 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 128.5/375.7 MB 11.6 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 131.1/375.7 MB 11.6 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 133.4/375.7 MB 11.6 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 136.1/375.7 MB 11.6 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 138.7/375.7 MB 11.6 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 141.3/375.7 MB 11.6 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 143.7/375.7 MB 11.6 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 146.0/375.7 MB 11.6 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 148.6/375.7 MB 11.7 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 151.3/375.7 MB 11.6 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 153.6/375.7 MB 11.7 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 156.2/375.7 MB 11.6 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 158.6/375.7 MB 11.7 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 161.2/375.7 MB 11.7 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 163.6/375.7 MB 11.7 MB/s eta 0:00:19\n",
      "   ----------------- --------------------- 166.2/375.7 MB 11.7 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 168.6/375.7 MB 11.7 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 171.2/375.7 MB 11.7 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 173.8/375.7 MB 11.7 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 176.2/375.7 MB 11.7 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 178.8/375.7 MB 11.7 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 181.1/375.7 MB 11.7 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 183.8/375.7 MB 11.7 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 186.1/375.7 MB 11.7 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 188.7/375.7 MB 11.7 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 191.4/375.7 MB 11.7 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 193.7/375.7 MB 11.7 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 196.3/375.7 MB 11.7 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 198.7/375.7 MB 11.7 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 201.3/375.7 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 203.9/375.7 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 206.6/375.7 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 208.9/375.7 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 211.6/375.7 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 214.2/375.7 MB 11.7 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 216.5/375.7 MB 11.7 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 219.2/375.7 MB 11.7 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 221.8/375.7 MB 11.7 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 224.1/375.7 MB 11.7 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 226.8/375.7 MB 11.7 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 229.4/375.7 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 231.7/375.7 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 234.1/375.7 MB 11.7 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 236.7/375.7 MB 11.7 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 239.3/375.7 MB 11.7 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 241.7/375.7 MB 11.7 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 244.3/375.7 MB 11.7 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 246.9/375.7 MB 11.7 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 249.3/375.7 MB 11.7 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 251.9/375.7 MB 11.7 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 254.5/375.7 MB 11.7 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 257.2/375.7 MB 11.7 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 259.5/375.7 MB 11.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 262.1/375.7 MB 11.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 264.8/375.7 MB 11.8 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 267.1/375.7 MB 11.8 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 269.7/375.7 MB 11.8 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 272.4/375.7 MB 11.8 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 274.7/375.7 MB 11.8 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 277.3/375.7 MB 11.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 280.0/375.7 MB 11.8 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 282.3/375.7 MB 11.8 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 285.0/375.7 MB 11.8 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 287.6/375.7 MB 11.8 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 289.9/375.7 MB 11.8 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 292.6/375.7 MB 11.8 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 295.2/375.7 MB 11.8 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 297.5/375.7 MB 11.8 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 300.2/375.7 MB 11.8 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 302.8/375.7 MB 11.8 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 305.1/375.7 MB 11.8 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 307.8/375.7 MB 11.8 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 310.4/375.7 MB 11.8 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 312.7/375.7 MB 11.8 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 315.4/375.7 MB 11.8 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 318.0/375.7 MB 11.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 320.3/375.7 MB 11.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 323.0/375.7 MB 11.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 325.6/375.7 MB 11.8 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 327.9/375.7 MB 11.8 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 330.6/375.7 MB 11.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 333.2/375.7 MB 11.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 335.8/375.7 MB 11.8 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 337.4/375.7 MB 11.8 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 340.8/375.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 343.4/375.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 346.0/375.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 348.4/375.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 351.3/375.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 353.6/375.7 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 356.3/375.7 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 358.9/375.7 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 361.2/375.7 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 363.9/375.7 MB 11.8 MB/s eta 0:00:02\n",
      "   --------------------------------------  366.5/375.7 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  368.8/375.7 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  371.5/375.7 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.1/375.7 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 375.7/375.7 MB 11.6 MB/s  0:00:32\n",
      "Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.9 MB 8.5 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/12.9 MB 8.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/12.9 MB 907.1 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/12.9 MB 907.1 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/12.9 MB 907.1 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/12.9 MB 907.1 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/12.9 MB 613.9 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 1.0/12.9 MB 613.9 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 1.0/12.9 MB 613.9 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 1.0/12.9 MB 613.9 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 1.0/12.9 MB 613.9 kB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 438.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 438.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 438.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 438.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 438.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 368.0 kB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 368.0 kB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 368.0 kB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 368.0 kB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 368.0 kB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 334.4 kB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 334.4 kB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 334.4 kB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 334.4 kB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 334.4 kB/s eta 0:00:33\n",
      "   ------ --------------------------------- 2.1/12.9 MB 320.9 kB/s eta 0:00:34\n",
      "   ------ --------------------------------- 2.1/12.9 MB 320.9 kB/s eta 0:00:34\n",
      "   ------- -------------------------------- 2.4/12.9 MB 343.3 kB/s eta 0:00:31\n",
      "   -------- ------------------------------- 2.6/12.9 MB 371.0 kB/s eta 0:00:28\n",
      "   -------- ------------------------------- 2.9/12.9 MB 399.5 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 3.1/12.9 MB 428.2 kB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 483.6 kB/s eta 0:00:20\n",
      "   ------------- -------------------------- 4.2/12.9 MB 540.1 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 4.7/12.9 MB 592.9 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 645.3 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 696.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.3/12.9 MB 744.9 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 7.1/12.9 MB 818.4 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.9/12.9 MB 889.5 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.7/12.9 MB 958.7 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 9.7/12.9 MB 1.0 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 10.5/12.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------------  12.6/12.9 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 1.3 MB/s  0:00:09\n",
      "Using cached grpcio-1.75.1-cp310-cp310-win_amd64.whl (4.6 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp310-cp310-win_amd64.whl (206 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.5 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.5 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 5.4 MB/s  0:00:01\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached fonttools-4.60.1-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.3/2.9 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 6.2 MB/s  0:00:00\n",
      "Using cached keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Using cached onnx-1.17.0-cp310-cp310-win_amd64.whl (14.5 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pillow-12.0.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached wrapt-1.17.3-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.17.0-cp310-cp310-win_amd64.whl (304 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, pyparsing, protobuf, pillow, optree, opt_einsum, numpy, mdurl, MarkupSafe, markdown, kiwisolver, idna, grpcio, google_pasta, gast, fonttools, cycler, charset_normalizer, certifi, astunparse, absl-py, werkzeug, requests, opencv-python, onnx, ml-dtypes, markdown-it-py, h5py, contourpy, tf2onnx, tensorboard, rich, matplotlib, keras, tensorflow\n",
      "\n",
      "    ---------------------------------------  1/42 [libclang]\n",
      "    ---------------------------------------  1/42 [libclang]\n",
      "   --- ------------------------------------  4/42 [urllib3]\n",
      "   ------- --------------------------------  8/42 [pyparsing]\n",
      "   -------- -------------------------------  9/42 [protobuf]\n",
      "   --------- ------------------------------ 10/42 [pillow]\n",
      "   --------- ------------------------------ 10/42 [pillow]\n",
      "   --------- ------------------------------ 10/42 [pillow]\n",
      "   ----------- ---------------------------- 12/42 [opt_einsum]\n",
      "  Attempting uninstall: numpy\n",
      "   ----------- ---------------------------- 12/42 [opt_einsum]\n",
      "    Found existing installation: numpy 2.2.6\n",
      "   ----------- ---------------------------- 12/42 [opt_einsum]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "    Uninstalling numpy-2.2.6:\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   ------------ --------------------------- 13/42 [numpy]\n",
      "   --------------- ------------------------ 16/42 [markdown]\n",
      "   ------------------ --------------------- 19/42 [grpcio]\n",
      "   ------------------- -------------------- 20/42 [google_pasta]\n",
      "   -------------------- ------------------- 22/42 [fonttools]\n",
      "   -------------------- ------------------- 22/42 [fonttools]\n",
      "   -------------------- ------------------- 22/42 [fonttools]\n",
      "   -------------------- ------------------- 22/42 [fonttools]\n",
      "   -------------------- ------------------- 22/42 [fonttools]\n",
      "   -------------------- ------------------- 22/42 [fonttools]\n",
      "   -------------------- ------------------- 22/42 [fonttools]\n",
      "   ----------------------- ---------------- 25/42 [certifi]\n",
      "   -------------------------- ------------- 28/42 [werkzeug]\n",
      "   --------------------------- ------------ 29/42 [requests]\n",
      "   ---------------------------- ----------- 30/42 [opencv-python]\n",
      "   ---------------------------- ----------- 30/42 [opencv-python]\n",
      "   ---------------------------- ----------- 30/42 [opencv-python]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ----------------------------- ---------- 31/42 [onnx]\n",
      "   ------------------------------- -------- 33/42 [markdown-it-py]\n",
      "   -------------------------------- ------- 34/42 [h5py]\n",
      "   --------------------------------- ------ 35/42 [contourpy]\n",
      "   ---------------------------------- ----- 36/42 [tf2onnx]\n",
      "   ---------------------------------- ----- 36/42 [tf2onnx]\n",
      "   ---------------------------------- ----- 36/42 [tf2onnx]\n",
      "   ---------------------------------- ----- 36/42 [tf2onnx]\n",
      "   ----------------------------------- ---- 37/42 [tensorboard]\n",
      "   ----------------------------------- ---- 37/42 [tensorboard]\n",
      "   ----------------------------------- ---- 37/42 [tensorboard]\n",
      "   ----------------------------------- ---- 37/42 [tensorboard]\n",
      "   ----------------------------------- ---- 37/42 [tensorboard]\n",
      "   ------------------------------------ --- 38/42 [rich]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   ------------------------------------- -- 39/42 [matplotlib]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   -------------------------------------- - 40/42 [keras]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------  41/42 [tensorflow]\n",
      "   ---------------------------------------- 42/42 [tensorflow]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 absl-py-2.3.1 astunparse-1.6.3 certifi-2025.10.5 charset_normalizer-3.4.4 contourpy-1.3.2 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.60.1 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.1 h5py-3.15.1 idna-3.11 keras-3.11.3 kiwisolver-1.4.9 libclang-18.1.1 markdown-3.9 markdown-it-py-4.0.0 matplotlib-3.10.7 mdurl-0.1.2 ml-dtypes-0.5.3 namex-0.1.0 numpy-2.1.3 onnx-1.17.0 opencv-python-4.12.0.88 opt_einsum-3.4.0 optree-0.17.0 pillow-12.0.0 protobuf-3.20.3 pyparsing-3.2.5 requests-2.32.5 rich-14.2.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 tf2onnx-1.16.1 urllib3-2.5.0 werkzeug-3.1.3 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "%pip install tf2onnx matplotlib numpy opencv-python scipy tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed27fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting tf2onnx\n",
      "  Using cached tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.21-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Using cached protobuf-6.33.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.3-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.75.1-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Using cached keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.15.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached pillow-12.0.0-cp310-cp310-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting onnx>=1.4.1 (from tf2onnx)\n",
      "  Using cached onnx-1.19.1-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "INFO: pip is looking at multiple versions of tf2onnx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tf2onnx\n",
      "  Using cached tf2onnx-1.16.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.15.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.15.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.14.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.13.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.12.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.12.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "INFO: pip is still looking at multiple versions of tf2onnx to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached tf2onnx-1.11.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.10.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.9.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.9.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.9.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached tf2onnx-1.8.5-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Using cached tf2onnx-1.8.4-py3-none-any.whl.metadata (390 bytes)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.1-cp310-cp310-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting attrs>=19.1.0 (from mediapipe)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.6.2-cp310-cp310-win_amd64.whl.metadata (1.4 kB)\n",
      "INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.20-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "  Using cached mediapipe-0.10.18-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "  Using cached mediapipe-0.10.14-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.13-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "  Using cached mediapipe-0.10.11-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "  Using cached mediapipe-0.10.10-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "  Using cached mediapipe-0.10.9-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "INFO: pip is still looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached mediapipe-0.10.8-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "  Using cached mediapipe-0.10.7-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "  Using cached mediapipe-0.10.5-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.15.2-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached scipy-1.15.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.15.0-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.14.0-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.13.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.13.0-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.12.0-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached scipy-1.11.4-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.11.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "  Using cached scipy-1.11.1-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "  Using cached scipy-1.10.1-cp310-cp310-win_amd64.whl.metadata (58 kB)\n",
      "  Using cached scipy-1.10.0-cp310-cp310-win_amd64.whl.metadata (58 kB)\n",
      "  Using cached scipy-1.9.3-cp310-cp310-win_amd64.whl.metadata (58 kB)\n",
      "INFO: pip is still looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached scipy-1.9.2-cp310-cp310-win_amd64.whl.metadata (58 kB)\n",
      "  Using cached scipy-1.9.1-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "  Using cached scipy-1.9.0-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "  Using cached scipy-1.8.1-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "  Using cached scipy-1.8.0-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached scipy-1.7.3-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "  Using cached scipy-1.7.2-1-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "  Using cached scipy-1.6.1.tar.gz (27.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "   Preparing metadata (pyproject.toml) did not run successfully.\n",
      "   exit code: 1\n",
      "  > [146 lines of output]\n",
      "      setup.py:461: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\pip-modern-metadata-qgz7mycq'), proceeding with generating Cython sources and expanding templates\n",
      "        warnings.warn(\"Unrecognized setuptools command ('{}'), proceeding with \"\n",
      "      setup.py:563: DeprecationWarning:\n",
      "      \n",
      "        `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "        of the deprecation of `distutils` itself. It will be removed for\n",
      "        Python >= 3.12. For older Python versions it will remain present.\n",
      "        It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "        For more details, see:\n",
      "          https://numpy.org/devdocs/reference/distutils_status_migration.html\n",
      "      \n",
      "      \n",
      "        from numpy.distutils.core import setup\n",
      "      Running from SciPy source directory.\n",
      "      INFO: lapack_opt_info:\n",
      "      INFO: lapack_armpl_info:\n",
      "      INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "      INFO: customize MSVCCompiler\n",
      "      INFO:   libraries armpl_lp64_mp not found in ['c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\lib', 'C:\\\\', 'c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\libs', 'C:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\Library\\\\lib']\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: lapack_mkl_info:\n",
      "      INFO:   libraries mkl_rt not found in ['c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\lib', 'C:\\\\', 'c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\libs', 'C:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\Library\\\\lib']\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: lapack_ssl2_info:\n",
      "      INFO:   libraries fjlapackexsve not found in ['c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\lib', 'C:\\\\', 'c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\libs', 'C:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\Library\\\\lib']\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: openblas_lapack_info:\n",
      "      INFO:   libraries openblas not found in ['c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\lib', 'C:\\\\', 'c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\libs', 'C:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\Library\\\\lib']\n",
      "      INFO: get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'\n",
      "      INFO: customize GnuFCompiler\n",
      "      WARN: Could not locate executable g77\n",
      "      WARN: Could not locate executable f77\n",
      "      INFO: customize IntelVisualFCompiler\n",
      "      WARN: Could not locate executable ifort\n",
      "      WARN: Could not locate executable ifl\n",
      "      INFO: customize AbsoftFCompiler\n",
      "      WARN: Could not locate executable f90\n",
      "      INFO: customize CompaqVisualFCompiler\n",
      "      WARN: Could not locate executable DF\n",
      "      INFO: customize IntelItaniumVisualFCompiler\n",
      "      WARN: Could not locate executable efl\n",
      "      INFO: customize Gnu95FCompiler\n",
      "      INFO: Found executable C:\\MinGW\\bin\\gfortran.exe\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: openblas_clapack_info:\n",
      "      INFO:   libraries openblas,lapack not found in ['c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\lib', 'C:\\\\', 'c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\libs', 'C:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\Library\\\\lib']\n",
      "      INFO: get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'\n",
      "      INFO: customize GnuFCompiler\n",
      "      INFO: customize IntelVisualFCompiler\n",
      "      INFO: customize AbsoftFCompiler\n",
      "      INFO: customize CompaqVisualFCompiler\n",
      "      INFO: customize IntelItaniumVisualFCompiler\n",
      "      INFO: customize Gnu95FCompiler\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: flame_info:\n",
      "      INFO:   libraries flame not found in ['c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\lib', 'C:\\\\', 'c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\libs', 'C:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\Library\\\\lib']\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: accelerate_info:\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: atlas_3_10_threads_info:\n",
      "      INFO: Setting PTATLAS=ATLAS\n",
      "      INFO:   libraries tatlas,tatlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\lib\n",
      "      INFO:   libraries tatlas,tatlas not found in C:\\\n",
      "      INFO:   libraries tatlas,tatlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\libs\n",
      "      INFO:   libraries tatlas,tatlas not found in C:\\Users\\WINDOWS 10\\anaconda3\\Library\\lib\n",
      "      INFO: <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: atlas_3_10_info:\n",
      "      INFO:   libraries satlas,satlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\lib\n",
      "      INFO:   libraries satlas,satlas not found in C:\\\n",
      "      INFO:   libraries satlas,satlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\libs\n",
      "      INFO:   libraries satlas,satlas not found in C:\\Users\\WINDOWS 10\\anaconda3\\Library\\lib\n",
      "      INFO: <class 'numpy.distutils.system_info.atlas_3_10_info'>\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: atlas_threads_info:\n",
      "      INFO: Setting PTATLAS=ATLAS\n",
      "      INFO:   libraries ptf77blas,ptcblas,atlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\lib\n",
      "      INFO:   libraries ptf77blas,ptcblas,atlas not found in C:\\\n",
      "      INFO:   libraries ptf77blas,ptcblas,atlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\libs\n",
      "      INFO:   libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\WINDOWS 10\\anaconda3\\Library\\lib\n",
      "      INFO: <class 'numpy.distutils.system_info.atlas_threads_info'>\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: atlas_info:\n",
      "      INFO:   libraries f77blas,cblas,atlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\lib\n",
      "      INFO:   libraries f77blas,cblas,atlas not found in C:\\\n",
      "      INFO:   libraries f77blas,cblas,atlas not found in c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\libs\n",
      "      INFO:   libraries f77blas,cblas,atlas not found in C:\\Users\\WINDOWS 10\\anaconda3\\Library\\lib\n",
      "      INFO: <class 'numpy.distutils.system_info.atlas_info'>\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      INFO: lapack_info:\n",
      "      INFO:   libraries lapack not found in ['c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\lib', 'C:\\\\', 'c:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\envs\\\\aienv\\\\libs', 'C:\\\\Users\\\\WINDOWS 10\\\\anaconda3\\\\Library\\\\lib']\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\pip-build-env-aqver941\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:1973: UserWarning:\n",
      "          Lapack (http://www.netlib.org/lapack/) libraries not found.\n",
      "          Directories to search for the libraries can be specified in the\n",
      "          numpy/distutils/site.cfg file (section [lapack]) or by setting\n",
      "          the LAPACK environment variable.\n",
      "        return getattr(self, '_calc_info_{}'.format(name))()\n",
      "      INFO: lapack_src_info:\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\pip-build-env-aqver941\\overlay\\Lib\\site-packages\\numpy\\distutils\\system_info.py:1973: UserWarning:\n",
      "          Lapack (http://www.netlib.org/lapack/) sources not found.\n",
      "          Directories to search for the sources can be specified in the\n",
      "          numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n",
      "          the LAPACK_SRC environment variable.\n",
      "        return getattr(self, '_calc_info_{}'.format(name))()\n",
      "      INFO:   NOT AVAILABLE\n",
      "      INFO:\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "        File \"c:\\Users\\WINDOWS 10\\anaconda3\\envs\\aienv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 175, in prepare_metadata_for_build_wheel\n",
      "          return hook(metadata_directory, config_settings)\n",
      "        File \"C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\pip-build-env-aqver941\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 161, in prepare_metadata_for_build_wheel\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\pip-build-env-aqver941\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 253, in run_setup\n",
      "          super(_BuildMetaLegacyBackend,\n",
      "        File \"C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\pip-build-env-aqver941\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 145, in run_setup\n",
      "          exec(compile(code, __file__, 'exec'), locals())\n",
      "        File \"setup.py\", line 588, in <module>\n",
      "          setup_package()\n",
      "        File \"setup.py\", line 584, in setup_package\n",
      "          setup(**metadata)\n",
      "        File \"C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\pip-build-env-aqver941\\overlay\\Lib\\site-packages\\numpy\\distutils\\core.py\", line 135, in setup\n",
      "          config = configuration()\n",
      "        File \"setup.py\", line 499, in configuration\n",
      "          raise NotFoundError(msg)\n",
      "      numpy.distutils.system_info.NotFoundError: No BLAS/LAPACK libraries found.\n",
      "      To build Scipy from sources, BLAS & LAPACK libraries need to be installed.\n",
      "      See site.cfg.example in the Scipy source directory and\n",
      "      https://docs.scipy.org/doc/scipy/reference/building/index.html for details.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      " Encountered error while generating package metadata.\n",
      "> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall tensorflow tf2onnx matplotlib numpy opencv-python scipy mediapipe pandas tqdm scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908a35ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70d0a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnx\n",
      "  Using cached tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting numpy>=1.14.1 (from tf2onnx)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting onnx>=1.4.1 (from tf2onnx)\n",
      "  Using cached onnx-1.19.1-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting requests (from tf2onnx)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting six (from tf2onnx)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting flatbuffers>=1.12 (from tf2onnx)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf~=3.20 (from tf2onnx)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl.metadata (698 bytes)\n",
      "INFO: pip is looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting onnx>=1.4.1 (from tf2onnx)\n",
      "  Using cached onnx-1.19.0-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "  Using cached onnx-1.18.0-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "  Using cached onnx-1.17.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->tf2onnx)\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->tf2onnx)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->tf2onnx)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->tf2onnx)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Using cached onnx-1.17.0-cp310-cp310-win_amd64.whl (14.5 MB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: flatbuffers, urllib3, six, protobuf, numpy, idna, charset_normalizer, certifi, requests, onnx, tf2onnx\n",
      "\n",
      "  Attempting uninstall: flatbuffers\n",
      "\n",
      "    Found existing installation: flatbuffers 25.9.23\n",
      "\n",
      "    Uninstalling flatbuffers-25.9.23:\n",
      "\n",
      "      Successfully uninstalled flatbuffers-25.9.23\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "\n",
      "   --- ------------------------------------  1/11 [urllib3]\n",
      "  Attempting uninstall: six\n",
      "   --- ------------------------------------  1/11 [urllib3]\n",
      "    Found existing installation: six 1.17.0\n",
      "   --- ------------------------------------  1/11 [urllib3]\n",
      "    Uninstalling six-1.17.0:\n",
      "   --- ------------------------------------  1/11 [urllib3]\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "      Successfully uninstalled six-1.17.0\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "  Attempting uninstall: protobuf\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "   ------- --------------------------------  2/11 [six]\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "  Attempting uninstall: numpy\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "    Found existing installation: numpy 2.1.3\n",
      "   ---------- -----------------------------  3/11 [protobuf]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "    Uninstalling numpy-2.1.3:\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "  Attempting uninstall: idna\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "    Found existing installation: idna 3.11\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "    Uninstalling idna-3.11:\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "      Successfully uninstalled idna-3.11\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "  Attempting uninstall: charset_normalizer\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "    Found existing installation: charset-normalizer 3.4.4\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "    Uninstalling charset-normalizer-3.4.4:\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "      Successfully uninstalled charset-normalizer-3.4.4\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "  Attempting uninstall: certifi\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "    Found existing installation: certifi 2025.10.5\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "    Uninstalling certifi-2025.10.5:\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "      Successfully uninstalled certifi-2025.10.5\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "  Attempting uninstall: requests\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "    Found existing installation: requests 2.32.5\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "    Uninstalling requests-2.32.5:\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "      Successfully uninstalled requests-2.32.5\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "   ----------------------------- ----------  8/11 [requests]\n",
      "  Attempting uninstall: onnx\n",
      "   ----------------------------- ----------  8/11 [requests]\n",
      "    Found existing installation: onnx 1.17.0\n",
      "   ----------------------------- ----------  8/11 [requests]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "    Uninstalling onnx-1.17.0:\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "      Successfully uninstalled onnx-1.17.0\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "  Attempting uninstall: tf2onnx\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "    Found existing installation: tf2onnx 1.16.1\n",
      "   -------------------------------- -------  9/11 [onnx]\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "    Uninstalling tf2onnx-1.16.1:\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "      Successfully uninstalled tf2onnx-1.16.1\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "   ------------------------------------ --- 10/11 [tf2onnx]\n",
      "   ---------------------------------------- 11/11 [tf2onnx]\n",
      "\n",
      "Successfully installed certifi-2025.10.5 charset_normalizer-3.4.4 flatbuffers-25.9.23 idna-3.11 numpy-2.2.6 onnx-1.17.0 protobuf-3.20.3 requests-2.32.5 six-1.17.0 tf2onnx-1.16.1 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.1 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d18590",
   "metadata": {},
   "source": [
    "# import and constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc1fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 07:37:06.942527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760747826.971448    1550 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760747826.979649    1550 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760747827.069732    1550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760747827.069766    1550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760747827.069768    1550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760747827.069769    1550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-18 07:37:07.079763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from tensorflow.keras.applications import MobileNetV2 # type: ignore\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow.keras.preprocessing import image # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import cv2 # type: ignore\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import cv2 # type: ignore\n",
    "import mediapipe as mp # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.preprocessing import LabelEncoder # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "import joblib # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be719935",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './dataset'\n",
    "model_name = 'sign_classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13103e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766be0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./alfabet-bisindo.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/achmadnoer/alfabet-bisindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66037a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./alfabet-bisindo.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\" File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\" Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\" Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\" OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21028c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_dir = \"Citra BISINDO\"\n",
    "dst_dir = \"./dataset\"\n",
    "\n",
    "if not os.path.exists(src_dir):\n",
    "    raise FileNotFoundError(f\" Folder sumber tidak ditemukan: {src_dir}\")\n",
    "\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "print(f\" Memindahkan '{src_dir}' ke '{dst_dir}' ...\")\n",
    "shutil.move(src_dir, dst_dir)\n",
    "print(f\" Berhasil dipindahkan ke {dst_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./indonesian-sign-language-bisindo.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/agungmrf/indonesian-sign-language-bisindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure.\n",
    "    Example: src_folder/cat -> dataset/cat\n",
    "             src_folder/dog -> dataset/dog\n",
    "    \"\"\"\n",
    "    # Ensure destination exists\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over all label folders in the source\n",
    "    for label in os.listdir(src_folder):\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Skip if not a folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Create label folder in destination if needed\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Move all files from src  dest\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Avoid overwriting files with same name\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_2{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\" Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./indonesian-sign-language-bisindo.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\" File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\" Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\" Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\" OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset('./bisindo/images/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset('./bisindo/images/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1353535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./bisindo\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\" Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\" Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\" Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeabb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./bisindo-dataset.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/yunitayupratiwi/bisindo-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./bisindo-dataset.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\" File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\" Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\" Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\" OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, based on the first letter of the filename.\n",
    "    Example: src_folder/A.66ae97e2-c1e4-11eb-83d3-0008ca6b6d30.jpg -> dataset/A\n",
    "             src_folder/B.002d8fdf-c1e3-11eb-952a-0008ca6b6d30.jpg -> dataset/B\n",
    "    \"\"\"\n",
    "    # Ensure destination exists\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over all file folders in the source\n",
    "    for filename in os.listdir(src_folder):\n",
    "        src_file = os.path.join(src_folder, filename)\n",
    "\n",
    "        # Skip if a folder\n",
    "        if os.path.isdir(src_file):\n",
    "            continue\n",
    "        \n",
    "        # Skip if not a jpg\n",
    "        if not src_file.lower().endswith('.jpg'):\n",
    "            continue\n",
    "        \n",
    "        label = filename[0].upper()  # First character as label\n",
    "        dest = os.path.join(dataset_dir, label)\n",
    "        # Create label folder in destination if needed\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "        dst_file = os.path.join(dest, filename)\n",
    "\n",
    "        # Avoid overwriting files with same name\n",
    "        if os.path.exists(dst_file):\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            dst_file = os.path.join(dest, f\"{base}_3{ext}\")\n",
    "\n",
    "        shutil.move(src_file, dst_file)\n",
    "\n",
    "    print(f\"successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b15a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_dataset(\"./BISINDO - Dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf12b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_dataset(\"./BISINDO - Dataset/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./BISINDO - Dataset\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\" Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\" Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\" Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./sign-language-bisindo.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/bonarsitorus/sign-language-bisindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e87a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./sign-language-bisindo.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\" File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\" Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\" Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\" OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f178eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset_4(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure,\n",
    "    except folders with '_npy' in their name.\n",
    "    \"\"\"\n",
    "    # Pastikan folder tujuan ada\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterasi semua folder di src_folder\n",
    "    for label in os.listdir(src_folder):\n",
    "        # Lewati folder yang mengandung '_npy'\n",
    "        if \"_npy\" in label:\n",
    "            continue\n",
    "\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Pastikan ini folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Buat folder di tujuan jika belum ada\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Pindahkan semua file\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Hindari overwrite\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_4{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\" Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_4('./data_tambahan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3406b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./data_tambahan\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\" Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\" Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\" Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./indonesian-hand-sign-language-bisindo-dataset.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/kelsha/indonesian-hand-sign-language-bisindo-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./indonesian-hand-sign-language-bisindo-dataset.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\" File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "print(f\" Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\" Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\" OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset_5(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure.\n",
    "    \"\"\"\n",
    "    # Pastikan folder tujuan ada\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterasi semua folder di src_folder\n",
    "    for label in os.listdir(src_folder):\n",
    "\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Pastikan ini folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Buat folder di tujuan jika belum ada\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Pindahkan semua file\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Hindari overwrite\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_5{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\" Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_5('./dataset_bisindo/train/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d39e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_5('./dataset_bisindo/val/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1beb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./dataset_bisindo\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\" Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\" Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\" Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o ./bisindo-final.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/skripsiairlangga/bisindo-final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ca9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import platform\n",
    "\n",
    "zip_path = \"./bisindo-final.zip\"\n",
    "extract_dir = \"./bisindo_final\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    raise FileNotFoundError(f\" File tidak ditemukan: {zip_path}\")\n",
    "\n",
    "# Membuat folder extract_dir jika belum ada\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(f\" Ekstraksi dataset dari {zip_path} ...\")\n",
    "if platform.system() in [\"Windows\", \"Darwin\", \"Linux\"]:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\" Dataset berhasil diekstrak ke {extract_dir}\")\n",
    "else:\n",
    "    print(f\" OS {platform.system()} belum dikenali, pastikan unzip dilakukan manual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57255468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset_6(src_folder):\n",
    "    \"\"\"\n",
    "    Move files from src_folder into dataset, preserving subfolder (label) structure.\n",
    "    \"\"\"\n",
    "    # Pastikan folder tujuan ada\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Iterasi semua folder di src_folder\n",
    "    for label in os.listdir(src_folder):\n",
    "\n",
    "        label_path_src = os.path.join(src_folder, label)\n",
    "        label_path_dest = os.path.join(dataset_dir, label)\n",
    "\n",
    "        # Pastikan ini folder\n",
    "        if not os.path.isdir(label_path_src):\n",
    "            continue\n",
    "\n",
    "        # Buat folder di tujuan jika belum ada\n",
    "        os.makedirs(label_path_dest, exist_ok=True)\n",
    "\n",
    "        # Pindahkan semua file\n",
    "        for filename in os.listdir(label_path_src):\n",
    "            src = os.path.join(label_path_src, filename)\n",
    "            dst = os.path.join(label_path_dest, filename)\n",
    "\n",
    "            # Hindari overwrite\n",
    "            if os.path.exists(dst):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                dst = os.path.join(label_path_dest, f\"{base}_6{ext}\")\n",
    "\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    print(f\" Merged '{src_folder}' into '{dataset_dir}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce715f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_6('./bisindo_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = \"./bisindo_final\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\" Menghapus folder: {folder_path}\")\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\" Folder berhasil dihapus.\")\n",
    "else:\n",
    "    print(\" Folder tidak ditemukan, tidak ada yang dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4257553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(image_rgb)\n",
    "    \n",
    "    if not result.multi_hand_landmarks:\n",
    "        return None\n",
    "    \n",
    "    # List semua tangan yang terdeteksi\n",
    "    coords_all = []\n",
    "    for landmarks in result.multi_hand_landmarks:\n",
    "        coords = []\n",
    "        for lm in landmarks.landmark:\n",
    "            coords.extend([lm.x, lm.y, lm.z])\n",
    "        coords_all.append(coords)\n",
    "    \n",
    "    # Jika hanya 1 tangan, tambahkan 0 agar panjang fitur tetap konsisten\n",
    "    if len(coords_all) == 1:\n",
    "        coords_all.append([0.0]*63)  # tangan kosong\n",
    "    \n",
    "    # Flatten dua tangan (kanan + kiri)\n",
    "    features = np.array(coords_all[0] + coords_all[1])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for label in sorted(os.listdir(dataset_dir)):\n",
    "    label_path = os.path.join(dataset_dir, label)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    print(f'Processing label: {label}')\n",
    "    for img_name in tqdm(os.listdir(label_path)):\n",
    "        img_path = os.path.join(label_path, img_name)\n",
    "        landmarks = extract_hand_landmarks(img_path)\n",
    "        if landmarks is not None:\n",
    "            data.append(landmarks)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e16017",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"preprocessing\", exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['label'] = labels\n",
    "\n",
    "csv_path = os.path.join(\"preprocessing\", \"hand_keypoints.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\" Data dan label berhasil disimpan ke {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeba27f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9411987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 25305\n",
      "          0         1             2         3         4         5         6  \\\n",
      "0  0.589252  0.788577  7.732578e-08  0.540710  0.797574 -0.010122  0.494010   \n",
      "1  0.816977  0.762783 -9.139245e-08  0.801779  0.732191  0.003005  0.774102   \n",
      "2  0.725103  0.491935 -4.427249e-07  0.658864  0.500457 -0.027891  0.584359   \n",
      "3  0.188371  0.656543 -8.581127e-08  0.283549  0.646443 -0.036480  0.366026   \n",
      "4  0.113166  0.822599  5.401650e-07  0.195477  0.813338 -0.077608  0.288585   \n",
      "\n",
      "          7         8         9  ...       117       118       119       120  \\\n",
      "0  0.776065 -0.034902  0.458915  ...  0.259818  0.687959 -0.093514  0.256858   \n",
      "1  0.710470  0.000877  0.747481  ...  0.627653  0.740314 -0.056922  0.625121   \n",
      "2  0.476563 -0.053607  0.521293  ...  0.223792  0.330818 -0.103738  0.219856   \n",
      "3  0.589501 -0.066297  0.421726  ...  0.758943  0.368739 -0.107302  0.773216   \n",
      "4  0.775300 -0.115059  0.381177  ...  0.871711  0.542666 -0.079876  0.875374   \n",
      "\n",
      "        121       122       123       124       125  label  \n",
      "0  0.731087 -0.083111  0.242542  0.740920 -0.070938      A  \n",
      "1  0.737795 -0.057063  0.627400  0.744683 -0.054733      A  \n",
      "2  0.384147 -0.090373  0.220490  0.394726 -0.071387      A  \n",
      "3  0.424807 -0.090161  0.791487  0.425094 -0.069101      A  \n",
      "4  0.603269 -0.072792  0.862440  0.643383 -0.055686      A  \n",
      "\n",
      "[5 rows x 127 columns]\n"
     ]
    }
   ],
   "source": [
    "csv_input_path = os.path.join(\"preprocessing\", \"hand_keypoints.csv\")\n",
    "df = pd.read_csv(csv_input_path)\n",
    "print(f\"Total data: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['label'])\n",
    "X = df.drop('label', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f130e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9753c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e90ded",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db52f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(\"models\", f\"{model_name}.keras\")\n",
    "label_path = os.path.join(\"models\", f\"{model_name}_label.pkl\")\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "with open(label_path, \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(f\" Model disimpan di {model_path}\")\n",
    "print(f\" Label encoder disimpan di {label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf # type: ignore\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = os.path.join(\"models\", f\"{model_name}.tflite\")\n",
    "\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\" Model TFLite disimpan di {tflite_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "label_path = os.path.join(\"models\", f\"{model_name}_label.pkl\")\n",
    "with open(label_path, \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "labels = le.classes_\n",
    "\n",
    "txt_path = os.path.join(\"models\", \"labels.txt\")\n",
    "with open(txt_path, \"w\") as f:\n",
    "    for label in labels:\n",
    "        f.write(label + \"\\n\")\n",
    "\n",
    "print(f\" Labels berhasil disimpan di {txt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-rhn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
